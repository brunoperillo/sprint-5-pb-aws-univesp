{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UOxZh2Bjj4iw"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import itertools\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDTUKLrzj6MA",
        "outputId": "2206ffdb-d4c0-44f9-ba87-580cd7e85561"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3)\n",
            "y_test shape: (10000, 1)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64  # The default batch size of keras.\n",
        "num_classes = 10  # Number of class for the dataset\n",
        "epochs = 100\n",
        "data_augmentation = False"
      ],
      "metadata": {
        "id": "h7rK6Tx4k2al"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the data. Before we need to connvert data type to float for computation.\n",
        "#print(x_train[0])\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "#print(x_train[0])'"
      ],
      "metadata": {
        "id": "KbBEEYRPkIL2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert class vectors to binary class matrices. This is called one hot encoding.\n",
        "print(y_train[0])\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJcjF_UtQYjf",
        "outputId": "44bd3540-f8c7-4b92-ba44-7e37f58eeedc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert class vectors to binary class matrices. This is called one hot encoding.\n",
        "print(y_train[0])\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print(y_train[0])\n"
      ],
      "metadata": {
        "id": "7dtj2fGzkdD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "x_train = x_train.reshape((50000, 32 * 32 * 3))\n",
        "x_test = x_test.reshape((10000, 32 * 32 * 3))\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(32 * 32 * 3,)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=128)\n"
      ],
      "metadata": {
        "id": "aK8a0TWB0Z78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the convnet\n",
        "model = Sequential()\n",
        "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
        "model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# FLATTERN => DENSE => RELU => DROPOUT\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))     #Original era 50%\n",
        "\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))     #Original era 50%\n",
        "\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))     #Original era 50%\n",
        "\n",
        "# a softmax classifier\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfx2gV68kttz",
        "outputId": "5cc5abfb-9b22-4b49-85c9-ac8d8af4d090"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_8 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 30, 30, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 15, 15, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 15, 15, 32)        0         \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 15, 15, 64)        18496     \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 15, 15, 64)        0         \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 13, 13, 64)        36928     \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 13, 13, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 6, 6, 64)          0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 2304)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               295040    \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " activation_20 (Activation)  (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            " activation_21 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 394,922\n",
            "Trainable params: 394,922\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "FNxf0aaclKHf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = None  # For recording the history of trainning process.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    history = model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    history = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                    batch_size=batch_size),\n",
        "                                    epochs=epochs,\n",
        "                                    validation_data=(x_test, y_test),\n",
        "                                    workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzND_i-TlNqP",
        "outputId": "b8531541-559c-4295-9d31-5efc6c400541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using data augmentation.\n",
            "Epoch 1/100\n",
            "782/782 [==============================] - 287s 365ms/step - loss: 2.0556 - accuracy: 0.2221 - val_loss: 1.7981 - val_accuracy: 0.3372\n",
            "Epoch 2/100\n",
            "782/782 [==============================] - 278s 356ms/step - loss: 1.7629 - accuracy: 0.3442 - val_loss: 1.6831 - val_accuracy: 0.3958\n",
            "Epoch 3/100\n",
            "782/782 [==============================] - 288s 368ms/step - loss: 1.6293 - accuracy: 0.3978 - val_loss: 1.6231 - val_accuracy: 0.4187\n",
            "Epoch 4/100\n",
            "782/782 [==============================] - 274s 350ms/step - loss: 1.5301 - accuracy: 0.4416 - val_loss: 1.4344 - val_accuracy: 0.4705\n",
            "Epoch 5/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 1.4544 - accuracy: 0.4690 - val_loss: 1.3665 - val_accuracy: 0.5073\n",
            "Epoch 6/100\n",
            "782/782 [==============================] - 274s 350ms/step - loss: 1.3864 - accuracy: 0.4992 - val_loss: 1.2432 - val_accuracy: 0.5591\n",
            "Epoch 7/100\n",
            "782/782 [==============================] - 284s 363ms/step - loss: 1.3222 - accuracy: 0.5243 - val_loss: 1.2051 - val_accuracy: 0.5733\n",
            "Epoch 8/100\n",
            "782/782 [==============================] - 284s 363ms/step - loss: 1.2669 - accuracy: 0.5464 - val_loss: 1.1424 - val_accuracy: 0.5978\n",
            "Epoch 9/100\n",
            "782/782 [==============================] - 282s 361ms/step - loss: 1.2199 - accuracy: 0.5639 - val_loss: 1.1103 - val_accuracy: 0.6097\n",
            "Epoch 10/100\n",
            "782/782 [==============================] - 273s 349ms/step - loss: 1.1653 - accuracy: 0.5844 - val_loss: 1.0606 - val_accuracy: 0.6275\n",
            "Epoch 11/100\n",
            "782/782 [==============================] - 273s 348ms/step - loss: 1.1241 - accuracy: 0.6034 - val_loss: 1.0572 - val_accuracy: 0.6261\n",
            "Epoch 12/100\n",
            "782/782 [==============================] - 274s 351ms/step - loss: 1.0866 - accuracy: 0.6168 - val_loss: 1.0042 - val_accuracy: 0.6434\n",
            "Epoch 13/100\n",
            "782/782 [==============================] - 285s 365ms/step - loss: 1.0511 - accuracy: 0.6296 - val_loss: 0.9802 - val_accuracy: 0.6562\n",
            "Epoch 14/100\n",
            "782/782 [==============================] - 285s 364ms/step - loss: 1.0192 - accuracy: 0.6430 - val_loss: 0.9548 - val_accuracy: 0.6605\n",
            "Epoch 15/100\n",
            "782/782 [==============================] - 285s 364ms/step - loss: 0.9876 - accuracy: 0.6543 - val_loss: 0.9288 - val_accuracy: 0.6789\n",
            "Epoch 16/100\n",
            "782/782 [==============================] - 288s 368ms/step - loss: 0.9626 - accuracy: 0.6638 - val_loss: 0.8893 - val_accuracy: 0.6885\n",
            "Epoch 17/100\n",
            "782/782 [==============================] - 285s 364ms/step - loss: 0.9415 - accuracy: 0.6723 - val_loss: 0.9083 - val_accuracy: 0.6840\n",
            "Epoch 18/100\n",
            "782/782 [==============================] - 286s 365ms/step - loss: 0.9185 - accuracy: 0.6800 - val_loss: 0.8868 - val_accuracy: 0.6904\n",
            "Epoch 19/100\n",
            "782/782 [==============================] - 273s 349ms/step - loss: 0.8965 - accuracy: 0.6899 - val_loss: 0.8352 - val_accuracy: 0.7099\n",
            "Epoch 20/100\n",
            "782/782 [==============================] - 284s 363ms/step - loss: 0.8740 - accuracy: 0.6962 - val_loss: 0.8765 - val_accuracy: 0.6979\n",
            "Epoch 21/100\n",
            "782/782 [==============================] - 273s 349ms/step - loss: 0.8523 - accuracy: 0.7046 - val_loss: 0.8480 - val_accuracy: 0.7099\n",
            "Epoch 22/100\n",
            "782/782 [==============================] - 282s 360ms/step - loss: 0.8390 - accuracy: 0.7094 - val_loss: 0.7991 - val_accuracy: 0.7241\n",
            "Epoch 23/100\n",
            "782/782 [==============================] - 272s 348ms/step - loss: 0.8188 - accuracy: 0.7162 - val_loss: 0.7797 - val_accuracy: 0.7311\n",
            "Epoch 24/100\n",
            "782/782 [==============================] - 272s 348ms/step - loss: 0.8019 - accuracy: 0.7235 - val_loss: 0.8033 - val_accuracy: 0.7225\n",
            "Epoch 25/100\n",
            "782/782 [==============================] - 271s 347ms/step - loss: 0.7830 - accuracy: 0.7304 - val_loss: 0.7947 - val_accuracy: 0.7234\n",
            "Epoch 26/100\n",
            "782/782 [==============================] - 281s 359ms/step - loss: 0.7713 - accuracy: 0.7334 - val_loss: 0.7473 - val_accuracy: 0.7405\n",
            "Epoch 27/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.7580 - accuracy: 0.7392 - val_loss: 0.7371 - val_accuracy: 0.7454\n",
            "Epoch 28/100\n",
            "782/782 [==============================] - 282s 361ms/step - loss: 0.7436 - accuracy: 0.7441 - val_loss: 0.7261 - val_accuracy: 0.7466\n",
            "Epoch 29/100\n",
            "782/782 [==============================] - 272s 348ms/step - loss: 0.7341 - accuracy: 0.7465 - val_loss: 0.7313 - val_accuracy: 0.7495\n",
            "Epoch 30/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.7163 - accuracy: 0.7529 - val_loss: 0.7066 - val_accuracy: 0.7558\n",
            "Epoch 31/100\n",
            "782/782 [==============================] - 275s 352ms/step - loss: 0.7070 - accuracy: 0.7566 - val_loss: 0.7253 - val_accuracy: 0.7510\n",
            "Epoch 32/100\n",
            "782/782 [==============================] - 273s 349ms/step - loss: 0.6940 - accuracy: 0.7608 - val_loss: 0.7063 - val_accuracy: 0.7603\n",
            "Epoch 33/100\n",
            "782/782 [==============================] - 275s 352ms/step - loss: 0.6800 - accuracy: 0.7672 - val_loss: 0.6989 - val_accuracy: 0.7632\n",
            "Epoch 34/100\n",
            "782/782 [==============================] - 273s 349ms/step - loss: 0.6740 - accuracy: 0.7681 - val_loss: 0.7043 - val_accuracy: 0.7606\n",
            "Epoch 35/100\n",
            "782/782 [==============================] - 275s 351ms/step - loss: 0.6590 - accuracy: 0.7738 - val_loss: 0.6816 - val_accuracy: 0.7648\n",
            "Epoch 36/100\n",
            "782/782 [==============================] - 284s 363ms/step - loss: 0.6494 - accuracy: 0.7762 - val_loss: 0.6848 - val_accuracy: 0.7662\n",
            "Epoch 37/100\n",
            "782/782 [==============================] - 274s 350ms/step - loss: 0.6466 - accuracy: 0.7754 - val_loss: 0.7181 - val_accuracy: 0.7596\n",
            "Epoch 38/100\n",
            "782/782 [==============================] - 282s 360ms/step - loss: 0.6326 - accuracy: 0.7825 - val_loss: 0.6748 - val_accuracy: 0.7675\n",
            "Epoch 39/100\n",
            "782/782 [==============================] - 270s 346ms/step - loss: 0.6238 - accuracy: 0.7862 - val_loss: 0.6809 - val_accuracy: 0.7682\n",
            "Epoch 40/100\n",
            "782/782 [==============================] - 284s 363ms/step - loss: 0.6203 - accuracy: 0.7871 - val_loss: 0.6629 - val_accuracy: 0.7740\n",
            "Epoch 41/100\n",
            "782/782 [==============================] - 281s 359ms/step - loss: 0.6082 - accuracy: 0.7923 - val_loss: 0.6698 - val_accuracy: 0.7735\n",
            "Epoch 42/100\n",
            "782/782 [==============================] - 273s 349ms/step - loss: 0.6007 - accuracy: 0.7942 - val_loss: 0.6475 - val_accuracy: 0.7768\n",
            "Epoch 43/100\n",
            "782/782 [==============================] - 281s 359ms/step - loss: 0.5915 - accuracy: 0.7974 - val_loss: 0.6578 - val_accuracy: 0.7794\n",
            "Epoch 44/100\n",
            "782/782 [==============================] - 281s 360ms/step - loss: 0.5867 - accuracy: 0.7978 - val_loss: 0.6646 - val_accuracy: 0.7768\n",
            "Epoch 45/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.5797 - accuracy: 0.8003 - val_loss: 0.6429 - val_accuracy: 0.7853\n",
            "Epoch 46/100\n",
            "782/782 [==============================] - 285s 364ms/step - loss: 0.5688 - accuracy: 0.8045 - val_loss: 0.6290 - val_accuracy: 0.7871\n",
            "Epoch 47/100\n",
            "782/782 [==============================] - 282s 361ms/step - loss: 0.5647 - accuracy: 0.8061 - val_loss: 0.6557 - val_accuracy: 0.7777\n",
            "Epoch 48/100\n",
            "782/782 [==============================] - 280s 358ms/step - loss: 0.5620 - accuracy: 0.8065 - val_loss: 0.6360 - val_accuracy: 0.7837\n",
            "Epoch 49/100\n",
            "782/782 [==============================] - 269s 344ms/step - loss: 0.5535 - accuracy: 0.8113 - val_loss: 0.6613 - val_accuracy: 0.7795\n",
            "Epoch 50/100\n",
            "782/782 [==============================] - 281s 359ms/step - loss: 0.5501 - accuracy: 0.8114 - val_loss: 0.6342 - val_accuracy: 0.7873\n",
            "Epoch 51/100\n",
            "782/782 [==============================] - 279s 357ms/step - loss: 0.5403 - accuracy: 0.8145 - val_loss: 0.6233 - val_accuracy: 0.7882\n",
            "Epoch 52/100\n",
            "782/782 [==============================] - 273s 349ms/step - loss: 0.5337 - accuracy: 0.8164 - val_loss: 0.6702 - val_accuracy: 0.7816\n",
            "Epoch 53/100\n",
            "782/782 [==============================] - 280s 359ms/step - loss: 0.5302 - accuracy: 0.8184 - val_loss: 0.6603 - val_accuracy: 0.7833\n",
            "Epoch 54/100\n",
            "782/782 [==============================] - 270s 345ms/step - loss: 0.5248 - accuracy: 0.8195 - val_loss: 0.6273 - val_accuracy: 0.7917\n",
            "Epoch 55/100\n",
            "782/782 [==============================] - 280s 358ms/step - loss: 0.5184 - accuracy: 0.8238 - val_loss: 0.6541 - val_accuracy: 0.7833\n",
            "Epoch 56/100\n",
            "782/782 [==============================] - 268s 343ms/step - loss: 0.5153 - accuracy: 0.8210 - val_loss: 0.6246 - val_accuracy: 0.7889\n",
            "Epoch 57/100\n",
            "782/782 [==============================] - 278s 356ms/step - loss: 0.5083 - accuracy: 0.8249 - val_loss: 0.6438 - val_accuracy: 0.7826\n",
            "Epoch 58/100\n",
            "782/782 [==============================] - 279s 357ms/step - loss: 0.5048 - accuracy: 0.8272 - val_loss: 0.6202 - val_accuracy: 0.7928\n",
            "Epoch 59/100\n",
            "782/782 [==============================] - 270s 345ms/step - loss: 0.4991 - accuracy: 0.8279 - val_loss: 0.6677 - val_accuracy: 0.7821\n",
            "Epoch 60/100\n",
            "782/782 [==============================] - 277s 354ms/step - loss: 0.4926 - accuracy: 0.8327 - val_loss: 0.6540 - val_accuracy: 0.7886\n",
            "Epoch 61/100\n",
            "782/782 [==============================] - 280s 358ms/step - loss: 0.4910 - accuracy: 0.8314 - val_loss: 0.6502 - val_accuracy: 0.7889\n",
            "Epoch 62/100\n",
            "782/782 [==============================] - 281s 359ms/step - loss: 0.4864 - accuracy: 0.8324 - val_loss: 0.6489 - val_accuracy: 0.7882\n",
            "Epoch 63/100\n",
            "782/782 [==============================] - 270s 346ms/step - loss: 0.4807 - accuracy: 0.8351 - val_loss: 0.6297 - val_accuracy: 0.7947\n",
            "Epoch 64/100\n",
            "782/782 [==============================] - 270s 345ms/step - loss: 0.4768 - accuracy: 0.8357 - val_loss: 0.6220 - val_accuracy: 0.7953\n",
            "Epoch 65/100\n",
            "782/782 [==============================] - 270s 346ms/step - loss: 0.4753 - accuracy: 0.8380 - val_loss: 0.6202 - val_accuracy: 0.7965\n",
            "Epoch 66/100\n",
            "782/782 [==============================] - 278s 356ms/step - loss: 0.4666 - accuracy: 0.8398 - val_loss: 0.6111 - val_accuracy: 0.8025\n",
            "Epoch 67/100\n",
            "782/782 [==============================] - 270s 346ms/step - loss: 0.4648 - accuracy: 0.8407 - val_loss: 0.6253 - val_accuracy: 0.7943\n",
            "Epoch 68/100\n",
            "782/782 [==============================] - 279s 357ms/step - loss: 0.4620 - accuracy: 0.8407 - val_loss: 0.6312 - val_accuracy: 0.7918\n",
            "Epoch 69/100\n",
            "782/782 [==============================] - 269s 344ms/step - loss: 0.4617 - accuracy: 0.8427 - val_loss: 0.6158 - val_accuracy: 0.7949\n",
            "Epoch 70/100\n",
            "782/782 [==============================] - 269s 344ms/step - loss: 0.4578 - accuracy: 0.8454 - val_loss: 0.6324 - val_accuracy: 0.7961\n",
            "Epoch 71/100\n",
            "782/782 [==============================] - 280s 358ms/step - loss: 0.4504 - accuracy: 0.8451 - val_loss: 0.6068 - val_accuracy: 0.8022\n",
            "Epoch 72/100\n",
            "782/782 [==============================] - 270s 345ms/step - loss: 0.4513 - accuracy: 0.8473 - val_loss: 0.6014 - val_accuracy: 0.8009\n",
            "Epoch 73/100\n",
            "782/782 [==============================] - 270s 345ms/step - loss: 0.4442 - accuracy: 0.8488 - val_loss: 0.6378 - val_accuracy: 0.7947\n",
            "Epoch 74/100\n",
            "782/782 [==============================] - 280s 358ms/step - loss: 0.4387 - accuracy: 0.8496 - val_loss: 0.5923 - val_accuracy: 0.8066\n",
            "Epoch 75/100\n",
            "782/782 [==============================] - 271s 346ms/step - loss: 0.4410 - accuracy: 0.8474 - val_loss: 0.6474 - val_accuracy: 0.7939\n",
            "Epoch 76/100\n",
            " 74/782 [=>............................] - ETA: 3:54 - loss: 0.4240 - accuracy: 0.8549"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plotmodelhistory(history): \n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(history.history['accuracy']) \n",
        "    axs[0].plot(history.history['val_accuracy']) \n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy') \n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend(['train', 'validate'], loc='upper left')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(history.history['loss']) \n",
        "    axs[1].plot(history.history['val_loss']) \n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss') \n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend(['train', 'validate'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "\n",
        "plotmodelhistory(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "quMOTQAQlQqV",
        "outputId": "bf95b69a-1723-410f-8391-fc4a6335d01d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d35239ce4e8b>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# list all data in history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mplotmodelhistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p1LXxMdM79I0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}