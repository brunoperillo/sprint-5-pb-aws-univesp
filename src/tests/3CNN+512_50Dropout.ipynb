{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UOxZh2Bjj4iw"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import itertools\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDTUKLrzj6MA",
        "outputId": "566bd7e2-5b60-4020-fd3f-dfb7f2739b51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3)\n",
            "y_test shape: (10000, 1)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ],
      "source": [
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h7rK6Tx4k2al"
      },
      "outputs": [],
      "source": [
        "batch_size = 64  # The default batch size of keras.\n",
        "num_classes = 10  # Number of class for the dataset\n",
        "epochs = 100\n",
        "data_augmentation = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KbBEEYRPkIL2"
      },
      "outputs": [],
      "source": [
        "# Normalize the data. Before we need to connvert data type to float for computation.\n",
        "#print(x_train[0])\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "#print(x_train[0])'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJcjF_UtQYjf",
        "outputId": "e5432071-4be8-4d43-ba6b-6f06af308f48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Convert class vectors to binary class matrices. This is called one hot encoding.\n",
        "print(y_train[0])\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print(y_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfx2gV68kttz",
        "outputId": "324b366a-ce41-40b2-d42d-2bf787d2d099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 30, 30, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 15, 15, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 15, 15, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 13, 13, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 6, 6, 64)          0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 6, 6, 128)         73856     \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 6, 6, 128)         0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 4, 4, 128)         147584    \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 2, 2, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 2, 2, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               262656    \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 554,794\n",
            "Trainable params: 554,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#define the convnet\n",
        "model = Sequential()\n",
        "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
        "model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# ADDED THIRD CONV => RELU => POOL LAYER\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(128, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# FLATTERN => DENSE => RELU => DROPOUT\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5)) \n",
        "\n",
        "# a softmax classifier\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FNxf0aaclKHf"
      },
      "outputs": [],
      "source": [
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzND_i-TlNqP",
        "outputId": "304d4858-0e2b-4f0c-e2eb-8d1ce8339776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not using data augmentation.\n",
            "Epoch 1/100\n",
            "782/782 [==============================] - 299s 381ms/step - loss: 2.0374 - accuracy: 0.2292 - val_loss: 1.8027 - val_accuracy: 0.3419\n",
            "Epoch 2/100\n",
            "782/782 [==============================] - 288s 369ms/step - loss: 1.7417 - accuracy: 0.3553 - val_loss: 1.6429 - val_accuracy: 0.4033\n",
            "Epoch 3/100\n",
            "782/782 [==============================] - 299s 382ms/step - loss: 1.5757 - accuracy: 0.4207 - val_loss: 1.4800 - val_accuracy: 0.4630\n",
            "Epoch 4/100\n",
            "782/782 [==============================] - 296s 378ms/step - loss: 1.4649 - accuracy: 0.4633 - val_loss: 1.4503 - val_accuracy: 0.4754\n",
            "Epoch 5/100\n",
            "782/782 [==============================] - 285s 365ms/step - loss: 1.3808 - accuracy: 0.4998 - val_loss: 1.4449 - val_accuracy: 0.4769\n",
            "Epoch 6/100\n",
            "782/782 [==============================] - 294s 376ms/step - loss: 1.3061 - accuracy: 0.5271 - val_loss: 1.2077 - val_accuracy: 0.5655\n",
            "Epoch 7/100\n",
            "782/782 [==============================] - 294s 376ms/step - loss: 1.2445 - accuracy: 0.5536 - val_loss: 1.2611 - val_accuracy: 0.5505\n",
            "Epoch 8/100\n",
            "782/782 [==============================] - 294s 375ms/step - loss: 1.1872 - accuracy: 0.5771 - val_loss: 1.1671 - val_accuracy: 0.5739\n",
            "Epoch 9/100\n",
            "782/782 [==============================] - 285s 364ms/step - loss: 1.1398 - accuracy: 0.5961 - val_loss: 1.1235 - val_accuracy: 0.6054\n",
            "Epoch 10/100\n",
            "782/782 [==============================] - 293s 375ms/step - loss: 1.0963 - accuracy: 0.6114 - val_loss: 1.1274 - val_accuracy: 0.5958\n",
            "Epoch 11/100\n",
            "782/782 [==============================] - 284s 364ms/step - loss: 1.0570 - accuracy: 0.6250 - val_loss: 0.9837 - val_accuracy: 0.6485\n",
            "Epoch 12/100\n",
            "782/782 [==============================] - 293s 375ms/step - loss: 1.0235 - accuracy: 0.6377 - val_loss: 1.0006 - val_accuracy: 0.6443\n",
            "Epoch 13/100\n",
            "782/782 [==============================] - 293s 375ms/step - loss: 0.9929 - accuracy: 0.6483 - val_loss: 1.0143 - val_accuracy: 0.6454\n",
            "Epoch 14/100\n",
            "782/782 [==============================] - 294s 376ms/step - loss: 0.9591 - accuracy: 0.6609 - val_loss: 0.9255 - val_accuracy: 0.6722\n",
            "Epoch 15/100\n",
            "782/782 [==============================] - 293s 375ms/step - loss: 0.9364 - accuracy: 0.6711 - val_loss: 1.0295 - val_accuracy: 0.6344\n",
            "Epoch 16/100\n",
            "782/782 [==============================] - 295s 377ms/step - loss: 0.9100 - accuracy: 0.6785 - val_loss: 0.8674 - val_accuracy: 0.6924\n",
            "Epoch 17/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.8869 - accuracy: 0.6878 - val_loss: 0.9036 - val_accuracy: 0.6887\n",
            "Epoch 18/100\n",
            "782/782 [==============================] - 283s 361ms/step - loss: 0.8661 - accuracy: 0.6952 - val_loss: 0.8238 - val_accuracy: 0.7163\n",
            "Epoch 19/100\n",
            "782/782 [==============================] - 292s 374ms/step - loss: 0.8390 - accuracy: 0.7051 - val_loss: 0.8441 - val_accuracy: 0.7059\n",
            "Epoch 20/100\n",
            "782/782 [==============================] - 293s 374ms/step - loss: 0.8209 - accuracy: 0.7114 - val_loss: 0.7869 - val_accuracy: 0.7245\n",
            "Epoch 21/100\n",
            "782/782 [==============================] - 292s 374ms/step - loss: 0.8013 - accuracy: 0.7192 - val_loss: 0.7817 - val_accuracy: 0.7250\n",
            "Epoch 22/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.7832 - accuracy: 0.7249 - val_loss: 0.7754 - val_accuracy: 0.7342\n",
            "Epoch 23/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.7658 - accuracy: 0.7329 - val_loss: 0.7263 - val_accuracy: 0.7467\n",
            "Epoch 24/100\n",
            "782/782 [==============================] - 284s 363ms/step - loss: 0.7476 - accuracy: 0.7374 - val_loss: 0.7411 - val_accuracy: 0.7447\n",
            "Epoch 25/100\n",
            "782/782 [==============================] - 293s 374ms/step - loss: 0.7264 - accuracy: 0.7470 - val_loss: 0.7190 - val_accuracy: 0.7575\n",
            "Epoch 26/100\n",
            "782/782 [==============================] - 285s 364ms/step - loss: 0.7103 - accuracy: 0.7513 - val_loss: 0.7783 - val_accuracy: 0.7361\n",
            "Epoch 27/100\n",
            "782/782 [==============================] - 282s 361ms/step - loss: 0.6991 - accuracy: 0.7559 - val_loss: 0.7295 - val_accuracy: 0.7496\n",
            "Epoch 28/100\n",
            "782/782 [==============================] - 283s 361ms/step - loss: 0.6854 - accuracy: 0.7610 - val_loss: 0.6897 - val_accuracy: 0.7602\n",
            "Epoch 29/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.6746 - accuracy: 0.7638 - val_loss: 0.6793 - val_accuracy: 0.7692\n",
            "Epoch 30/100\n",
            "782/782 [==============================] - 293s 374ms/step - loss: 0.6611 - accuracy: 0.7702 - val_loss: 0.7046 - val_accuracy: 0.7620\n",
            "Epoch 31/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.6480 - accuracy: 0.7743 - val_loss: 0.6361 - val_accuracy: 0.7834\n",
            "Epoch 32/100\n",
            "782/782 [==============================] - 292s 374ms/step - loss: 0.6308 - accuracy: 0.7804 - val_loss: 0.6336 - val_accuracy: 0.7804\n",
            "Epoch 33/100\n",
            "782/782 [==============================] - 300s 384ms/step - loss: 0.6245 - accuracy: 0.7830 - val_loss: 0.6976 - val_accuracy: 0.7646\n",
            "Epoch 34/100\n",
            "782/782 [==============================] - 288s 368ms/step - loss: 0.6201 - accuracy: 0.7845 - val_loss: 0.6510 - val_accuracy: 0.7772\n",
            "Epoch 35/100\n",
            "782/782 [==============================] - 292s 373ms/step - loss: 0.6084 - accuracy: 0.7895 - val_loss: 0.7173 - val_accuracy: 0.7592\n",
            "Epoch 36/100\n",
            "782/782 [==============================] - 292s 373ms/step - loss: 0.5995 - accuracy: 0.7927 - val_loss: 0.6421 - val_accuracy: 0.7808\n",
            "Epoch 37/100\n",
            "782/782 [==============================] - 291s 373ms/step - loss: 0.5837 - accuracy: 0.7957 - val_loss: 0.6266 - val_accuracy: 0.7874\n",
            "Epoch 38/100\n",
            "782/782 [==============================] - 282s 361ms/step - loss: 0.5804 - accuracy: 0.7969 - val_loss: 0.6060 - val_accuracy: 0.7947\n",
            "Epoch 39/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.5677 - accuracy: 0.8015 - val_loss: 0.6516 - val_accuracy: 0.7819\n",
            "Epoch 40/100\n",
            "782/782 [==============================] - 284s 363ms/step - loss: 0.5636 - accuracy: 0.8055 - val_loss: 0.6207 - val_accuracy: 0.7925\n",
            "Epoch 41/100\n",
            "782/782 [==============================] - 284s 363ms/step - loss: 0.5574 - accuracy: 0.8067 - val_loss: 0.6069 - val_accuracy: 0.7968\n",
            "Epoch 42/100\n",
            "782/782 [==============================] - 293s 374ms/step - loss: 0.5474 - accuracy: 0.8089 - val_loss: 0.6008 - val_accuracy: 0.7996\n",
            "Epoch 43/100\n",
            "782/782 [==============================] - 293s 374ms/step - loss: 0.5409 - accuracy: 0.8130 - val_loss: 0.6079 - val_accuracy: 0.7972\n",
            "Epoch 44/100\n",
            "782/782 [==============================] - 294s 375ms/step - loss: 0.5353 - accuracy: 0.8163 - val_loss: 0.5978 - val_accuracy: 0.8040\n",
            "Epoch 45/100\n",
            "782/782 [==============================] - 285s 364ms/step - loss: 0.5260 - accuracy: 0.8168 - val_loss: 0.5738 - val_accuracy: 0.8057\n",
            "Epoch 46/100\n",
            "782/782 [==============================] - 294s 376ms/step - loss: 0.5200 - accuracy: 0.8205 - val_loss: 0.6346 - val_accuracy: 0.7885\n",
            "Epoch 47/100\n",
            "782/782 [==============================] - 284s 363ms/step - loss: 0.5149 - accuracy: 0.8212 - val_loss: 0.6049 - val_accuracy: 0.7944\n",
            "Epoch 48/100\n",
            "782/782 [==============================] - 293s 375ms/step - loss: 0.5066 - accuracy: 0.8231 - val_loss: 0.6059 - val_accuracy: 0.7959\n",
            "Epoch 49/100\n",
            "782/782 [==============================] - 285s 364ms/step - loss: 0.5020 - accuracy: 0.8247 - val_loss: 0.5575 - val_accuracy: 0.8135\n",
            "Epoch 50/100\n",
            "782/782 [==============================] - 299s 383ms/step - loss: 0.4945 - accuracy: 0.8276 - val_loss: 0.5958 - val_accuracy: 0.8011\n",
            "Epoch 51/100\n",
            "782/782 [==============================] - 285s 365ms/step - loss: 0.4886 - accuracy: 0.8307 - val_loss: 0.5690 - val_accuracy: 0.8078\n",
            "Epoch 52/100\n",
            "782/782 [==============================] - 292s 373ms/step - loss: 0.4772 - accuracy: 0.8334 - val_loss: 0.5969 - val_accuracy: 0.8045\n",
            "Epoch 53/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.4780 - accuracy: 0.8325 - val_loss: 0.6143 - val_accuracy: 0.8000\n",
            "Epoch 54/100\n",
            "782/782 [==============================] - 290s 370ms/step - loss: 0.4720 - accuracy: 0.8362 - val_loss: 0.5940 - val_accuracy: 0.8048\n",
            "Epoch 55/100\n",
            "782/782 [==============================] - 285s 365ms/step - loss: 0.4658 - accuracy: 0.8365 - val_loss: 0.5514 - val_accuracy: 0.8167\n",
            "Epoch 56/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.4625 - accuracy: 0.8391 - val_loss: 0.5774 - val_accuracy: 0.8067\n",
            "Epoch 57/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.4543 - accuracy: 0.8419 - val_loss: 0.5762 - val_accuracy: 0.8092\n",
            "Epoch 58/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.4544 - accuracy: 0.8420 - val_loss: 0.5685 - val_accuracy: 0.8126\n",
            "Epoch 59/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.4508 - accuracy: 0.8445 - val_loss: 0.6668 - val_accuracy: 0.7878\n",
            "Epoch 60/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.4419 - accuracy: 0.8450 - val_loss: 0.5739 - val_accuracy: 0.8089\n",
            "Epoch 61/100\n",
            "782/782 [==============================] - 283s 363ms/step - loss: 0.4384 - accuracy: 0.8481 - val_loss: 0.5731 - val_accuracy: 0.8082\n",
            "Epoch 62/100\n",
            "782/782 [==============================] - 292s 374ms/step - loss: 0.4405 - accuracy: 0.8468 - val_loss: 0.5649 - val_accuracy: 0.8160\n",
            "Epoch 63/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.4385 - accuracy: 0.8479 - val_loss: 0.5581 - val_accuracy: 0.8158\n",
            "Epoch 64/100\n",
            "782/782 [==============================] - 284s 363ms/step - loss: 0.4273 - accuracy: 0.8537 - val_loss: 0.5961 - val_accuracy: 0.8065\n",
            "Epoch 65/100\n",
            "782/782 [==============================] - 284s 364ms/step - loss: 0.4209 - accuracy: 0.8547 - val_loss: 0.5569 - val_accuracy: 0.8186\n",
            "Epoch 66/100\n",
            "782/782 [==============================] - 293s 374ms/step - loss: 0.4210 - accuracy: 0.8555 - val_loss: 0.5447 - val_accuracy: 0.8217\n",
            "Epoch 67/100\n",
            "782/782 [==============================] - 283s 362ms/step - loss: 0.4166 - accuracy: 0.8543 - val_loss: 0.5355 - val_accuracy: 0.8243\n",
            "Epoch 68/100\n",
            "782/782 [==============================] - 293s 375ms/step - loss: 0.4156 - accuracy: 0.8545 - val_loss: 0.5575 - val_accuracy: 0.8177\n",
            "Epoch 69/100\n",
            "782/782 [==============================] - 293s 374ms/step - loss: 0.4126 - accuracy: 0.8557 - val_loss: 0.5717 - val_accuracy: 0.8169\n",
            "Epoch 70/100\n",
            "782/782 [==============================] - 292s 374ms/step - loss: 0.4093 - accuracy: 0.8602 - val_loss: 0.5559 - val_accuracy: 0.8194\n",
            "Epoch 71/100\n",
            "782/782 [==============================] - 288s 369ms/step - loss: 0.4026 - accuracy: 0.8615 - val_loss: 0.5791 - val_accuracy: 0.8111\n",
            "Epoch 72/100\n",
            "782/782 [==============================] - 288s 368ms/step - loss: 0.4010 - accuracy: 0.8608 - val_loss: 0.6009 - val_accuracy: 0.8111\n",
            "Epoch 73/100\n",
            "782/782 [==============================] - 304s 388ms/step - loss: 0.3977 - accuracy: 0.8613 - val_loss: 0.5418 - val_accuracy: 0.8212\n",
            "Epoch 74/100\n",
            "782/782 [==============================] - 306s 391ms/step - loss: 0.3942 - accuracy: 0.8643 - val_loss: 0.5446 - val_accuracy: 0.8253\n",
            "Epoch 75/100\n",
            "782/782 [==============================] - 312s 399ms/step - loss: 0.3916 - accuracy: 0.8647 - val_loss: 0.5387 - val_accuracy: 0.8256\n",
            "Epoch 76/100\n",
            "782/782 [==============================] - 295s 378ms/step - loss: 0.3873 - accuracy: 0.8655 - val_loss: 0.5569 - val_accuracy: 0.8141\n",
            "Epoch 77/100\n",
            "136/782 [====>.........................] - ETA: 3:47 - loss: 0.3762 - accuracy: 0.8679"
          ]
        }
      ],
      "source": [
        "history = None  # For recording the history of trainning process.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    history = model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    history = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                    batch_size=batch_size),\n",
        "                                    epochs=epochs,\n",
        "                                    validation_data=(x_test, y_test),\n",
        "                                    workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "quMOTQAQlQqV",
        "outputId": "bf95b69a-1723-410f-8391-fc4a6335d01d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d35239ce4e8b>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# list all data in history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mplotmodelhistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ],
      "source": [
        "def plotmodelhistory(history): \n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(history.history['accuracy']) \n",
        "    axs[0].plot(history.history['val_accuracy']) \n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy') \n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend(['train', 'validate'], loc='upper left')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(history.history['loss']) \n",
        "    axs[1].plot(history.history['val_loss']) \n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss') \n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend(['train', 'validate'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "\n",
        "plotmodelhistory(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1LXxMdM79I0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
