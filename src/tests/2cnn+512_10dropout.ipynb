{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UOxZh2Bjj4iw"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import itertools\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDTUKLrzj6MA",
        "outputId": "05183d40-d685-47de-d928-2d4c60c55ef8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3)\n",
            "y_test shape: (10000, 1)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ],
      "source": [
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h7rK6Tx4k2al"
      },
      "outputs": [],
      "source": [
        "batch_size = 32  # The default batch size of keras.\n",
        "num_classes = 10  # Number of class for the dataset\n",
        "epochs = 100\n",
        "data_augmentation = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KbBEEYRPkIL2"
      },
      "outputs": [],
      "source": [
        "# Normalize the data. Before we need to connvert data type to float for computation.\n",
        "#print(x_train[0])\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "#print(x_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dtj2fGzkdD7",
        "outputId": "bd4e770f-fa94-4182-a04c-b9f24ac55f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Convert class vectors to binary class matrices. This is called one hot encoding.\n",
        "print(y_train[0])\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print(y_train[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfx2gV68kttz",
        "outputId": "050a5e60-74c1-4148-e2e1-a71e52b1fcdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 30, 30, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 15, 15, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 15, 15, 32)        0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 15, 15, 64)        0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 13, 13, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 6, 6, 64)          0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2304)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               1180160   \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,250,858\n",
            "Trainable params: 1,250,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#define the convnet\n",
        "model = Sequential()\n",
        "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
        "model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# FLATTERN => DENSE => RELU => DROPOUT\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))     \n",
        "# a softmax classifier\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FNxf0aaclKHf"
      },
      "outputs": [],
      "source": [
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzND_i-TlNqP",
        "outputId": "acda3106-f2bf-4aeb-b6b3-8ab49572eab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not using data augmentation.\n",
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 310s 198ms/step - loss: 1.7059 - accuracy: 0.3749 - val_loss: 1.4251 - val_accuracy: 0.4848\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 295s 189ms/step - loss: 1.3991 - accuracy: 0.4967 - val_loss: 1.3364 - val_accuracy: 0.5158\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 297s 190ms/step - loss: 1.2588 - accuracy: 0.5542 - val_loss: 1.3550 - val_accuracy: 0.5290\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 297s 190ms/step - loss: 1.1570 - accuracy: 0.5926 - val_loss: 1.1142 - val_accuracy: 0.6097\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 304s 194ms/step - loss: 1.0730 - accuracy: 0.6208 - val_loss: 1.0211 - val_accuracy: 0.6443\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 304s 194ms/step - loss: 1.0095 - accuracy: 0.6447 - val_loss: 0.9793 - val_accuracy: 0.6653\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 303s 194ms/step - loss: 0.9505 - accuracy: 0.6688 - val_loss: 0.9025 - val_accuracy: 0.6877\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 303s 194ms/step - loss: 0.9012 - accuracy: 0.6864 - val_loss: 0.9013 - val_accuracy: 0.6823\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 296s 189ms/step - loss: 0.8582 - accuracy: 0.7002 - val_loss: 0.8469 - val_accuracy: 0.7059\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 303s 194ms/step - loss: 0.8220 - accuracy: 0.7150 - val_loss: 0.8374 - val_accuracy: 0.7097\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 296s 189ms/step - loss: 0.7881 - accuracy: 0.7263 - val_loss: 0.7871 - val_accuracy: 0.7280\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 296s 189ms/step - loss: 0.7542 - accuracy: 0.7378 - val_loss: 0.7553 - val_accuracy: 0.7386\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 295s 189ms/step - loss: 0.7267 - accuracy: 0.7473 - val_loss: 0.7676 - val_accuracy: 0.7370\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 296s 190ms/step - loss: 0.7022 - accuracy: 0.7557 - val_loss: 0.7164 - val_accuracy: 0.7541\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 303s 194ms/step - loss: 0.6752 - accuracy: 0.7647 - val_loss: 0.7408 - val_accuracy: 0.7487\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 304s 195ms/step - loss: 0.6563 - accuracy: 0.7735 - val_loss: 0.6992 - val_accuracy: 0.7588\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 296s 189ms/step - loss: 0.6304 - accuracy: 0.7829 - val_loss: 0.6862 - val_accuracy: 0.7665\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 294s 188ms/step - loss: 0.6139 - accuracy: 0.7878 - val_loss: 0.6871 - val_accuracy: 0.7646\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 304s 194ms/step - loss: 0.5960 - accuracy: 0.7932 - val_loss: 0.7290 - val_accuracy: 0.7576\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 304s 195ms/step - loss: 0.5780 - accuracy: 0.8002 - val_loss: 0.6855 - val_accuracy: 0.7668\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 296s 190ms/step - loss: 0.5623 - accuracy: 0.8059 - val_loss: 0.6463 - val_accuracy: 0.7773\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 303s 194ms/step - loss: 0.5475 - accuracy: 0.8122 - val_loss: 0.6610 - val_accuracy: 0.7755\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 294s 188ms/step - loss: 0.5332 - accuracy: 0.8157 - val_loss: 0.6697 - val_accuracy: 0.7769\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 305s 195ms/step - loss: 0.5170 - accuracy: 0.8224 - val_loss: 0.6423 - val_accuracy: 0.7855\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 304s 194ms/step - loss: 0.5050 - accuracy: 0.8267 - val_loss: 0.6395 - val_accuracy: 0.7818\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 303s 194ms/step - loss: 0.4937 - accuracy: 0.8309 - val_loss: 0.6394 - val_accuracy: 0.7857\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 303s 194ms/step - loss: 0.4845 - accuracy: 0.8307 - val_loss: 0.6683 - val_accuracy: 0.7825\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 295s 189ms/step - loss: 0.4740 - accuracy: 0.8357 - val_loss: 0.6420 - val_accuracy: 0.7848\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 296s 189ms/step - loss: 0.4611 - accuracy: 0.8423 - val_loss: 0.6353 - val_accuracy: 0.7917\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 303s 194ms/step - loss: 0.4573 - accuracy: 0.8430 - val_loss: 0.6316 - val_accuracy: 0.7916\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 303s 194ms/step - loss: 0.4463 - accuracy: 0.8476 - val_loss: 0.6195 - val_accuracy: 0.7945\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 294s 188ms/step - loss: 0.4382 - accuracy: 0.8490 - val_loss: 0.6707 - val_accuracy: 0.7875\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 296s 190ms/step - loss: 0.4304 - accuracy: 0.8543 - val_loss: 0.6380 - val_accuracy: 0.7912\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 297s 190ms/step - loss: 0.4273 - accuracy: 0.8543 - val_loss: 0.7060 - val_accuracy: 0.7747\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 295s 189ms/step - loss: 0.4228 - accuracy: 0.8547 - val_loss: 0.6390 - val_accuracy: 0.7920\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 301s 193ms/step - loss: 0.4152 - accuracy: 0.8596 - val_loss: 0.6475 - val_accuracy: 0.7913\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 294s 188ms/step - loss: 0.4094 - accuracy: 0.8598 - val_loss: 0.6408 - val_accuracy: 0.7897\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 296s 189ms/step - loss: 0.4069 - accuracy: 0.8625 - val_loss: 0.6295 - val_accuracy: 0.7965\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 304s 195ms/step - loss: 0.3993 - accuracy: 0.8639 - val_loss: 0.6480 - val_accuracy: 0.7883\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 303s 194ms/step - loss: 0.3925 - accuracy: 0.8673 - val_loss: 0.6662 - val_accuracy: 0.7905\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 295s 189ms/step - loss: 0.3940 - accuracy: 0.8657 - val_loss: 0.6353 - val_accuracy: 0.7943\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 296s 189ms/step - loss: 0.3806 - accuracy: 0.8699 - val_loss: 0.6453 - val_accuracy: 0.7948\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 306s 196ms/step - loss: 0.3802 - accuracy: 0.8703 - val_loss: 0.6458 - val_accuracy: 0.7951\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 305s 195ms/step - loss: 0.3772 - accuracy: 0.8735 - val_loss: 0.6443 - val_accuracy: 0.8022\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 297s 190ms/step - loss: 0.3757 - accuracy: 0.8740 - val_loss: 0.6332 - val_accuracy: 0.7964\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 303s 194ms/step - loss: 0.3716 - accuracy: 0.8739 - val_loss: 0.7199 - val_accuracy: 0.7879\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 295s 189ms/step - loss: 0.3670 - accuracy: 0.8762 - val_loss: 0.6791 - val_accuracy: 0.7971\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 299s 192ms/step - loss: 0.3637 - accuracy: 0.8786 - val_loss: 0.6507 - val_accuracy: 0.7989\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 305s 195ms/step - loss: 0.3581 - accuracy: 0.8803 - val_loss: 0.6697 - val_accuracy: 0.7983\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 297s 190ms/step - loss: 0.3548 - accuracy: 0.8800 - val_loss: 0.6298 - val_accuracy: 0.8013\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 304s 194ms/step - loss: 0.3507 - accuracy: 0.8831 - val_loss: 0.6232 - val_accuracy: 0.8034\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 305s 195ms/step - loss: 0.3514 - accuracy: 0.8829 - val_loss: 0.6413 - val_accuracy: 0.8039\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 305s 195ms/step - loss: 0.3469 - accuracy: 0.8856 - val_loss: 0.6586 - val_accuracy: 0.7907\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 306s 196ms/step - loss: 0.3442 - accuracy: 0.8870 - val_loss: 0.6400 - val_accuracy: 0.8008\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 304s 195ms/step - loss: 0.3421 - accuracy: 0.8857 - val_loss: 0.6760 - val_accuracy: 0.8089\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 304s 195ms/step - loss: 0.3366 - accuracy: 0.8883 - val_loss: 0.6316 - val_accuracy: 0.8008\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 0.3407 - accuracy: 0.8887"
          ]
        }
      ],
      "source": [
        "history = None  # For recording the history of trainning process.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    history = model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    history = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                    batch_size=batch_size),\n",
        "                                    epochs=epochs,\n",
        "                                    validation_data=(x_test, y_test),\n",
        "                                    workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quMOTQAQlQqV"
      },
      "outputs": [],
      "source": [
        "def plotmodelhistory(history): \n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(history.history['accuracy']) \n",
        "    axs[0].plot(history.history['val_accuracy']) \n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy') \n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend(['train', 'validate'], loc='upper left')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(history.history['loss']) \n",
        "    axs[1].plot(history.history['val_loss']) \n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss') \n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend(['train', 'validate'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "\n",
        "plotmodelhistory(history)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
